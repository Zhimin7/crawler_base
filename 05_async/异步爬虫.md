首先爬虫是IO密集型的任务，假如使用requests库莱爬取某个站点，当发出第一个请求之后，程序必须等待网站返回响应，才能接着运行，而在等待响应的过程中，整个爬虫程序是一直在等待的状态，实际上没有做任何事情。对于这种情况，需要使用异步爬虫来解决上述问题。

# 一、协程

## 1.1 案例引入

在介绍协程之前，我们先来引入一个简单的案例。我们将对地址为：http://www.httpbin.org/delay/5的URL进行访问，访问该链接需要5秒才能响应，假如使用requests库对该URL发起100次请求的话，那么至少也要500秒才能全部访问完毕。那么接下来我们将对上述说明进行测试。

```python
import requests
import logging
import time


logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s %(filename)s %(levelname)s %(message)s',
                    datefmt='%a %d %b %Y %H:%M:%S',
                    filename='log.log',
                    filemode='a')

TOTAL_NUMBER = 100
url = 'http://www.httpbin.org/delay/5'

start_time = time.time()
for _ in range(1, TOTAL_NUMBER+1):
    logging.info('scraping %s', url)
    response = requests.get(url)
end_time = time.time()
logging.info('total time %s seconds', end_time - start_time)
```

这里，我直接使用循环构造100次请求，使用的是requests单线程对该网站进行访问，然而每次访问都至少需要等待5秒才能加载，因此访问100次该页面，加上网站的负载问题、网络问题，至少也要500秒才能全部访问完毕。

运行结果如下：

```
Tue 05 Jul 2022 09:42:15 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
Tue 05 Jul 2022 09:42:20 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
Tue 05 Jul 2022 09:42:26 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
...
Tue 05 Jul 2022 09:51:10 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
Tue 05 Jul 2022 09:51:15 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
Tue 05 Jul 2022 09:51:21 requests_demo1.py INFO scraping http://www.httpbin.org/delay/5
Tue 05 Jul 2022 09:51:26 requests_demo1.py INFO total time 551.6228368282318 seconds
```

## 1.2 基础知识

- **阻塞**

阻塞指的是程序在计算时未得到所需要的资源被挂起的一种状态，当程序被挂起时，自身无法完成其他事情，则称该程序在操作上是阻塞的。

例如，网络阻塞、IO阻塞、磁盘阻塞等都是常见的阻塞。阻塞的问题无处不在，包括CPU上下文切换时，所有的进程都无法真正干活，它们也会被阻塞。在多核CPU的情况下，正在执行上下文切换的时候，核也是不可使用的。

- **非阻塞**

程序在等待某个操作的过程中，程序还可以继续做其他的事情，则该程序在该操作上是非阻塞的。

非阻塞仅存在于封装的程序中包含独立的子程序单元。

非阻塞是为了解决阻塞问题而出现，正是因为阻塞而导致运行时耗增加而效率低下，我们才需要将阻塞改为非阻塞。

- **同步**

不同的程序单元为了解决同一个任务，往往需要通过某种通信方式保持一致，强制使每一个程序单元都按顺序执行。

简而言之，同步即有序。

- **异步**

不同的程序单元为了解决同一个任务，也可以不需要通过通信方式进行协调，那么这样每个程序单元则不是同步执行。

例如，下载网页。调度程序调用下载任务之后，即可继续调度其他的任务

简而言之，异步即无序。

- **多进程**

多进程是利用多核的优势，在同一时间内执行多个任务，可以大大提高执行的效率。

- **协程**

协程拥有自己的寄存器和栈。协程在调度切换的时候，将寄存器上下文和栈保存在其他的地方，等切换回来的时候，再恢复之前保存的寄存器和栈。因此，协程可以保持上一次调度后的状态，每一次重新进入时可以重新进入上一次的调度状态。

协程的本质是单进程，相对于多进程来说，它没有线程上下文切换的开销。

其实协程很好理解，举个例子，在爬虫场景下，我们对一个网站发出一次请求之后，在一段时间内没有得到响应，在等待响应的过程中程序可以调度其他的任务，直到请求响应之后再切换回原来的任务的原来状态。

- **协程的使用方法**

python中协程最常见的库是`asyncio`，首先需要了解以下几个概念：

- event_loop：事件循环，相当于一个无限循环，我们把函数注册到事件循环上，当满足发生条件时，就调用处理方法。
- coroutine：在python中指的是协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用，我们可以使用`async`这个关键字来定义一个方法，这个成绩再调用时不会立即被执行，而是返回一个协程对象。
- task：任务，这是对协程对象进一步的封装，包含协程对象的各个状态。
- future：代表将来执行或者没有执行的任务的结果，实际上和task没有什么区别。

## 1.3 定义协程

```python
import asyncio


async def execute(x):
    print('Number: ', x)

coroutine = execute(1)
print('coroutine:', coroutine)
print('After calling execute')

loop = asyncio.get_event_loop()
loop.run_until_complete(coroutine)
print('After calling loop')
```

首先我们引入一个`asyncio`这个包，这个才可以使用`async`、`await`关键字，然后使`async`定义了一个`excute`方法，该方法接收一个参数x，执行之后会打印该数字。

接下来，我们直接调用`excute`方法，但是并没有直接运行，而是返回了一个coroutine协程对象，随后使用`get_event_loop`方法创建一个事件循环loop，并调用loop对象的`run_until_complete`方法将协程对象注册到事件函数中，接着启动即可。

由此，不难看出`async`定义的一个方法，直接调用时无法运行，必须将此对象注册到事件循环函数中才能执行。

前面我们还介绍了task，它是对协程对象的一种封装，比协程对象多了运行状态，例如：running、finished等。

```python
import asyncio


async def execute(x):
    print('Number: ', x)

coroutine = execute(10)
print('coroutine:', coroutine)
print('After calling execute')

loop = asyncio.get_event_loop()
task = loop.create_task(coroutine)
print('Task:' , task)
loop.run_until_complete(task)
print('Task:' , task)
print('After calling loop')
```

定义task对象还有另一种方式，就是直接调用asyncio包的ensure_future方法，返回的结果也是task对象。

```python
import asyncio
from types import coroutine


async def execute(x):
    print('Number:', x)

coroutine = execute(10)

task = asyncio.ensure_future(coroutine)
print('Task: ', task)
loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task', task)
print('After calling loop')
```

## 1.4 绑定回调

